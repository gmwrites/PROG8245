{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 10 - NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to first try to implement some NLP techniques manually, then we are going check available libraries that perform the same task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = [\"Natural@ ( U.S. Language Processing, or NLP, is a fascinating field that focuses on enabling computers to understand and work with human language.\",\n",
    "\"To do this, we often # break down sentences or texts into smaller units called tokens.\",\n",
    "\"These tokens can be as @simple as individual words or even sub-word units.\",\n",
    "\"Tokenization is a crucial step in NLP because it allows us to analyze and process text data more effectively.\",\n",
    "\"Tokenization would split sentences into individual words.\",\n",
    "\"This breakdown forms the foundation for various NLP tasks, from sentiment analysis to machine translation.\",\n",
    "\"Understanding tokenization is the first step in unlocking the power of NLP.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to create a list of special characters that may be encountered in the paragraph\n",
    "special_characters = ['!','\"','#','$','%','&','_','(',')','*','+','/',':','','<','=','>','@','[','\\\\',']','^','`','{','|','}','~','\\t', '\\s']\n",
    "# TODO: Then we need to remove the special characters \n",
    "\n",
    "# Should we remove the dots and commas or include it as a token alone? DISCUSS AND APPLY!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Now we need to change sentences to tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to discuss one of the most known NLP Library, NLTK. <br>\n",
    "First we need to install NLTK Library:<br>\n",
    "Run - pip install nltk - in the terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\asus\\documents\\aaiml\\foundations of machine learning frameworks\\venv\\cscn8010_classical_ml\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\asus\\documents\\aaiml\\foundations of machine learning frameworks\\venv\\cscn8010_classical_ml\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\asus\\documents\\aaiml\\foundations of machine learning frameworks\\venv\\cscn8010_classical_ml\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\asus\\documents\\aaiml\\foundations of machine learning frameworks\\venv\\cscn8010_classical_ml\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\documents\\aaiml\\foundations of machine learning frameworks\\venv\\cscn8010_classical_ml\\lib\\site-packages (from nltk) (4.66.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\documents\\aaiml\\foundations of machine learning frameworks\\venv\\cscn8010_classical_ml\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK: It is a python library that can we used to perform all the NLP tasks(stemming, lemmatization, etc..)<br>\n",
    "In this class, we are going to learn how to do the following: <br>\n",
    "- Tokenization\n",
    "- Stop-word removal\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "- Finding Synonym/Antonym\n",
    "- POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.1\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Tokenization: \n",
    "Tokenization as discussed previously in class, is the process of dividing the whole text into tokens.<br>\n",
    "There are two main types of Tokenizers available in nltk library:<br>\n",
    "1. Sentence Tokenizer: Takes as an input a string (document) and returns a list of the sentences from the string, where each sentence is considered a token.\n",
    "2. Word Tokenizer: Takes as an input a string (document) and returns a list of words that make up the string, where each word is considered a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural@ ( U.S. Language Processing, or NLP, is a fascinating field that focuses on enabling computers to understand and work with human language. To do this, we often # break down sentences or texts into smaller units called tokens. These tokens can be as @simple as individual words or even sub-word units. Tokenization is a crucial step in NLP because it allows us to analyze and process text data more effectively. Tokenization would split sentences into individual words. This breakdown forms the foundation for various NLP tasks, from sentiment analysis to machine translation. Understanding tokenization is the first step in unlocking the power of NLP.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_string=\" \".join(paragraph)\n",
    "test_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural@ ( U.S.', 'Language Processing, or NLP, is a fascinating field that focuses on enabling computers to understand and work with human language.', 'To do this, we often # break down sentences or texts into smaller units called tokens.', 'These tokens can be as @simple as individual words or even sub-word units.', 'Tokenization is a crucial step in NLP because it allows us to analyze and process text data more effectively.', 'Tokenization would split sentences into individual words.', 'This breakdown forms the foundation for various NLP tasks, from sentiment analysis to machine translation.', 'Understanding tokenization is the first step in unlocking the power of NLP.']\n",
      "['Natural', '@', '(', 'U.S', '.', 'Language', 'Processing', ',', 'or', 'NLP', ',', 'is', 'a', 'fascinating', 'field', 'that', 'focuses', 'on', 'enabling', 'computers', 'to', 'understand', 'and', 'work', 'with', 'human', 'language', '.', 'To', 'do', 'this', ',', 'we', 'often', '#', 'break', 'down', 'sentences', 'or', 'texts', 'into', 'smaller', 'units', 'called', 'tokens', '.', 'These', 'tokens', 'can', 'be', 'as', '@', 'simple', 'as', 'individual', 'words', 'or', 'even', 'sub-word', 'units', '.', 'Tokenization', 'is', 'a', 'crucial', 'step', 'in', 'NLP', 'because', 'it', 'allows', 'us', 'to', 'analyze', 'and', 'process', 'text', 'data', 'more', 'effectively', '.', 'Tokenization', 'would', 'split', 'sentences', 'into', 'individual', 'words', '.', 'This', 'breakdown', 'forms', 'the', 'foundation', 'for', 'various', 'NLP', 'tasks', ',', 'from', 'sentiment', 'analysis', 'to', 'machine', 'translation', '.', 'Understanding', 'tokenization', 'is', 'the', 'first', 'step', 'in', 'unlocking', 'the', 'power', 'of', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "# Importing needed tokenizer functions\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "print(sent_tokenize(test_string))\n",
    "print(word_tokenize(test_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Natural',\n",
       "  '@',\n",
       "  '(',\n",
       "  'U.S',\n",
       "  '.',\n",
       "  'Language',\n",
       "  'Processing',\n",
       "  ',',\n",
       "  'or',\n",
       "  'NLP',\n",
       "  ',',\n",
       "  'is',\n",
       "  'a',\n",
       "  'fascinating',\n",
       "  'field',\n",
       "  'that',\n",
       "  'focuses',\n",
       "  'on',\n",
       "  'enabling',\n",
       "  'computers',\n",
       "  'to',\n",
       "  'understand',\n",
       "  'and',\n",
       "  'work',\n",
       "  'with',\n",
       "  'human',\n",
       "  'language',\n",
       "  '.'],\n",
       " ['To',\n",
       "  'do',\n",
       "  'this',\n",
       "  ',',\n",
       "  'we',\n",
       "  'often',\n",
       "  '#',\n",
       "  'break',\n",
       "  'down',\n",
       "  'sentences',\n",
       "  'or',\n",
       "  'texts',\n",
       "  'into',\n",
       "  'smaller',\n",
       "  'units',\n",
       "  'called',\n",
       "  'tokens',\n",
       "  '.'],\n",
       " ['These',\n",
       "  'tokens',\n",
       "  'can',\n",
       "  'be',\n",
       "  'as',\n",
       "  '@',\n",
       "  'simple',\n",
       "  'as',\n",
       "  'individual',\n",
       "  'words',\n",
       "  'or',\n",
       "  'even',\n",
       "  'sub-word',\n",
       "  'units',\n",
       "  '.'],\n",
       " ['Tokenization',\n",
       "  'is',\n",
       "  'a',\n",
       "  'crucial',\n",
       "  'step',\n",
       "  'in',\n",
       "  'NLP',\n",
       "  'because',\n",
       "  'it',\n",
       "  'allows',\n",
       "  'us',\n",
       "  'to',\n",
       "  'analyze',\n",
       "  'and',\n",
       "  'process',\n",
       "  'text',\n",
       "  'data',\n",
       "  'more',\n",
       "  'effectively',\n",
       "  '.'],\n",
       " ['Tokenization',\n",
       "  'would',\n",
       "  'split',\n",
       "  'sentences',\n",
       "  'into',\n",
       "  'individual',\n",
       "  'words',\n",
       "  '.'],\n",
       " ['This',\n",
       "  'breakdown',\n",
       "  'forms',\n",
       "  'the',\n",
       "  'foundation',\n",
       "  'for',\n",
       "  'various',\n",
       "  'NLP',\n",
       "  'tasks',\n",
       "  ',',\n",
       "  'from',\n",
       "  'sentiment',\n",
       "  'analysis',\n",
       "  'to',\n",
       "  'machine',\n",
       "  'translation',\n",
       "  '.'],\n",
       " ['Understanding',\n",
       "  'tokenization',\n",
       "  'is',\n",
       "  'the',\n",
       "  'first',\n",
       "  'step',\n",
       "  'in',\n",
       "  'unlocking',\n",
       "  'the',\n",
       "  'power',\n",
       "  'of',\n",
       "  'NLP',\n",
       "  '.']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens = [word_tokenize(sentence) for sentence in paragraph]\n",
    "word_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Stop-Words Removal:\n",
    "Stopwords as discussed previously in class, are words that are not important in the process of analyzing the data. <br>\n",
    "Example of stop-words could include: and, he, it ...<br>\n",
    "The main task is to remove all stop-words from the text before proceeding with preprocessing.<br>\n",
    "\n",
    "NLTK simplifies this task, by providing a list of stopwords, stored in nltk corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] 179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['arabic',\n",
       "  'azerbaijani',\n",
       "  'basque',\n",
       "  'bengali',\n",
       "  'catalan',\n",
       "  'chinese',\n",
       "  'danish',\n",
       "  'dutch',\n",
       "  'english',\n",
       "  'finnish',\n",
       "  'french',\n",
       "  'german',\n",
       "  'greek',\n",
       "  'hebrew',\n",
       "  'hinglish',\n",
       "  'hungarian',\n",
       "  'indonesian',\n",
       "  'italian',\n",
       "  'kazakh',\n",
       "  'nepali',\n",
       "  'norwegian',\n",
       "  'portuguese',\n",
       "  'romanian',\n",
       "  'russian',\n",
       "  'slovene',\n",
       "  'spanish',\n",
       "  'swedish',\n",
       "  'tajik',\n",
       "  'turkish'],\n",
       " 29)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.words('english'),len(stopwords.words('english')))\n",
    "\n",
    "stopwords.fileids(),len(stopwords.fileids())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should we remove the stop-words from a tokenized string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " '@',\n",
       " '(',\n",
       " 'U.S',\n",
       " '.',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " ',',\n",
       " 'NLP',\n",
       " ',',\n",
       " 'fascinating',\n",
       " 'field',\n",
       " 'focuses',\n",
       " 'enabling',\n",
       " 'computers',\n",
       " 'understand',\n",
       " 'work',\n",
       " 'human',\n",
       " 'language',\n",
       " '.',\n",
       " 'To',\n",
       " ',',\n",
       " 'often',\n",
       " '#',\n",
       " 'break',\n",
       " 'sentences',\n",
       " 'texts',\n",
       " 'smaller',\n",
       " 'units',\n",
       " 'called',\n",
       " 'tokens',\n",
       " '.',\n",
       " 'These',\n",
       " 'tokens',\n",
       " '@',\n",
       " 'simple',\n",
       " 'individual',\n",
       " 'words',\n",
       " 'even',\n",
       " 'sub-word',\n",
       " 'units',\n",
       " '.',\n",
       " 'Tokenization',\n",
       " 'crucial',\n",
       " 'step',\n",
       " 'NLP',\n",
       " 'allows',\n",
       " 'us',\n",
       " 'analyze',\n",
       " 'process',\n",
       " 'text',\n",
       " 'data',\n",
       " 'effectively',\n",
       " '.',\n",
       " 'Tokenization',\n",
       " 'would',\n",
       " 'split',\n",
       " 'sentences',\n",
       " 'individual',\n",
       " 'words',\n",
       " '.',\n",
       " 'This',\n",
       " 'breakdown',\n",
       " 'forms',\n",
       " 'foundation',\n",
       " 'various',\n",
       " 'NLP',\n",
       " 'tasks',\n",
       " ',',\n",
       " 'sentiment',\n",
       " 'analysis',\n",
       " 'machine',\n",
       " 'translation',\n",
       " '.',\n",
       " 'Understanding',\n",
       " 'tokenization',\n",
       " 'first',\n",
       " 'step',\n",
       " 'unlocking',\n",
       " 'power',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_removed = [word for word in word_tokenize(test_string) if word not in stopwords.words('english')]\n",
    "stop_words_removed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtask that accompanies stop-words removal, is the punctuation removal. As we previously saw how to remove stop-words, we need to remove the punctuation tokens in the text. This can be achieved by the help of string library accessing punctuation attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)\n",
    "## TODO: Save punctuation list to remove punctuation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove punctuation, the same block of code will be run for removing stop-words, but now replacing the stop-words list by punctuation list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Remove punctuation\n",
    "punctuation_removed = [word for word in stop_words_removed if word not in string.punctuation]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional task that accompanies stop-words removal and punctuation removal, is the Case-Folding. To Case-Fold our list for normalization it is pretty simple, we just need to call string.lower on every word as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Case-fold the words (lower-case)\n",
    "stop_words_punctuation_removed_case_folded = [word.lower() for word in punctuation_removed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming and Lemmatization:\n",
    "# --> Reach the root/lemma for each word\n",
    "# sentences --> root: Sentence\n",
    "# enabling --> root: enable\n",
    "# enabling --> after stemming: enabl\n",
    "# after lemmatization: enabling or enable\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming: \n",
    "Stemming as discussed in class, is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. <br>\n",
    "In simple words, we can say that stemming is the process of removing plural and adjectives from the word.<br>\n",
    "Stemming is made available in the NLTK Library, with the most used stemmer called PorterStemmer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\treebank.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you had an error of treebank not found, use this command and search in corpora for treebank then download\n",
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Original ******************************\n",
      "Pierre Vinken , 61 years old , will join the board as a nonexecutive\n",
      "director Nov. 29 . Mr. Vinken is chairman of Elsevier N.V. , the Dutch\n",
      "publishing group . Rudolph Agnew , 55 years old and former chairman of\n",
      "Consolidated Gold Fields PLC , was named *-1 a nonexecutive director\n",
      "of this British industrial conglomerate . A form of asbestos once used\n",
      "* * to make Kent cigarette filters has caused a high percentage of\n",
      "cancer deaths among a group of workers exposed * to it more than 30\n",
      "years ago , researchers reported 0 *T*-1 . The asbestos fiber ,\n",
      "crocidolite , is unusually resilient once it enters the lungs , with\n",
      "even brief exposures to it causing symptoms that *T*-1 show up decades\n",
      "later , researchers said 0 *T*-2 . Lorillard Inc. , the unit of New\n",
      "York-based Loews Corp. that *T*-2 makes Kent cigarettes , stopped\n",
      "using crocidolite in its Micronite cigarette filters in 1956 .\n",
      "Although preliminary findings were reported *-2 more than a year ago ,\n",
      "the latest results appear in today 's New England Journal of Medicine\n",
      ", a forum likely * to bring new attention to the problem . A Lorillard\n",
      "spokewoman said , `` This is an old story . We 're talking about years\n",
      "ago before anyone heard of asbestos having any questionable properties\n",
      ". There is no asbestos in our products now . '' Neither Lorillard nor\n",
      "the researchers who *T*-3 studied the workers were aware of any\n",
      "research on smokers of the Kent cigarettes . `` We have no useful\n",
      "information on whether users are at risk , '' said *T*-1 James A.\n",
      "Talcott of Boston 's Dana-Farber Cancer Institute . Dr. Talcott led a\n",
      "team of researchers from the National Cancer Institute and the medical\n",
      "schools of Harvard University and Boston University . The Lorillard\n",
      "spokeswoman said 0 asbestos was used *-1 in `` very modest amounts ''\n",
      "in * making paper for the filters in the early 1950s and replaced *-1\n",
      "with a different type of filter in 1956 . From 1953 to 1955 , 9.8\n",
      "billion Kent cigarettes with the filters were sold *-3 , the company\n",
      "said 0 *T*-1 . Among 33 men who *T*-4 worked closely with the\n",
      "substance , 28 *ICH*-1 have died -- more than three times the expected\n",
      "number . Four of the five surviving workers have asbestos-related\n",
      "diseases , including three with recently diagnosed cancer . The total\n",
      "of 18 deaths from malignant mesothelioma , lung cancer and asbestosis\n",
      "was far higher than * expected *?* , the researchers said 0 *T*-1 . ``\n",
      "The morbidity rate is a striking finding among those of us who *T*-5\n",
      "study asbestos-related diseases , '' said *T*-1 Dr. Talcott . The\n",
      "percentage of lung cancer deaths among the workers at the West Groton\n",
      ", Mass. , paper factory appears *-1 to be the highest for any asbestos\n",
      "workers studied * in Western industrialized countries , he said 0\n",
      "*T*-2 . The plant , which *T*-1 is owned *-4 by Hollingsworth & Vose\n",
      "Co. , was under contract *ICH*-2 with Lorillard * to make the\n",
      "cigarette filters . The finding probably will support those who *T*-6\n",
      "argue that the U.S. should regulate the class of asbestos including\n",
      "crocidolite more stringently than the common kind of asbestos ,\n",
      "chrysotile , found * in most schools and other buildings , Dr. Talcott\n",
      "said 0 *T*-1 . The U.S. is one of the few industrialized nations that\n",
      "*T*-7 does n't have a higher standard of regulation for the smooth ,\n",
      "needle-like fibers such as crocidolite that *T*-1 are classified *-5\n",
      "as amphobiles , according to Brooke T. Mossman , a professor of\n",
      "pathlogy at the University of Vermont College of Medicine . More\n",
      "common chrysotile fibers are curly and are more easily rejected *-1 by\n",
      "the body , Dr. Mossman explained 0 *T*-2 . In July , the Environmental\n",
      "Protection Agency imposed a gradual ban on virtually all uses of\n",
      "asbestos . By 1997 , almost all remaining uses of cancer-causing\n",
      "asbestos will be outlawed *-6 . About 160 workers at a factory that\n",
      "*T*-8 made paper for the Kent filters were exposed *-7 to asbestos in\n",
      "the 1950s . Areas of the factory *ICH*-2 were particularly dusty where\n",
      "the crocidolite was used *-8 *T*-1 . Workers dumped large burlap sacks\n",
      "of the imported material into a huge bin , poured in cotton and\n",
      "acetate fibers and mechanically mixed the dry fibers in a process used\n",
      "* * to make filters . Workers described `` clouds of blue dust '' that\n",
      "*T*-1 hung over parts of the factory , even though exhaust fans\n",
      "ventilated the area . `` There 's no question that some of those\n",
      "workers and managers contracted asbestos-related diseases , '' said\n",
      "*T*-1 Darrell Phillips , vice president of human resources for\n",
      "Hollingsworth & Vose . `` But you have *-1 to recognize that these\n",
      "events took place 35 years ago . It has no bearing on our work force\n",
      "today .\n",
      "****************************** Results *******************************\n",
      "pierr vinken , 61 year old , will join the board as a nonexecut\n",
      "director nov. 29 . mr. vinken is chairman of elsevi n.v. , the dutch\n",
      "publish group . rudolph agnew , 55 year old and former chairman of\n",
      "consolid gold field plc , wa name *-1 a nonexecut director of thi\n",
      "british industri conglomer . a form of asbesto onc use * * to make\n",
      "kent cigarett filter ha caus a high percentag of cancer death among a\n",
      "group of worker expos * to it more than 30 year ago , research report\n",
      "0 *t*-1 . the asbesto fiber , crocidolit , is unusu resili onc it\n",
      "enter the lung , with even brief exposur to it caus symptom that *t*-1\n",
      "show up decad later , research said 0 *t*-2 . lorillard inc. , the\n",
      "unit of new york-bas loew corp. that *t*-2 make kent cigarett , stop\n",
      "use crocidolit in it micronit cigarett filter in 1956 . although\n",
      "preliminari find were report *-2 more than a year ago , the latest\n",
      "result appear in today 's new england journal of medicin , a forum\n",
      "like * to bring new attent to the problem . a lorillard spokewoman\n",
      "said , `` thi is an old stori . we 're talk about year ago befor anyon\n",
      "heard of asbesto have ani question properti . there is no asbesto in\n",
      "our product now . '' neither lorillard nor the research who *t*-3\n",
      "studi the worker were awar of ani research on smoker of the kent\n",
      "cigarett . `` we have no use inform on whether user are at risk , ''\n",
      "said *t*-1 jame a. talcott of boston 's dana-farb cancer institut .\n",
      "dr. talcott led a team of research from the nation cancer institut and\n",
      "the medic school of harvard univers and boston univers . the lorillard\n",
      "spokeswoman said 0 asbesto wa use *-1 in `` veri modest amount '' in *\n",
      "make paper for the filter in the earli 1950 and replac *-1 with a\n",
      "differ type of filter in 1956 . from 1953 to 1955 , 9.8 billion kent\n",
      "cigarett with the filter were sold *-3 , the compani said 0 *t*-1 .\n",
      "among 33 men who *t*-4 work close with the substanc , 28 *ich*-1 have\n",
      "die -- more than three time the expect number . four of the five\n",
      "surviv worker have asbestos-rel diseas , includ three with recent\n",
      "diagnos cancer . the total of 18 death from malign mesothelioma , lung\n",
      "cancer and asbestosi wa far higher than * expect *?* , the research\n",
      "said 0 *t*-1 . `` the morbid rate is a strike find among those of us\n",
      "who *t*-5 studi asbestos-rel diseas , '' said *t*-1 dr. talcott . the\n",
      "percentag of lung cancer death among the worker at the west groton ,\n",
      "mass. , paper factori appear *-1 to be the highest for ani asbesto\n",
      "worker studi * in western industri countri , he said 0 *t*-2 . the\n",
      "plant , which *t*-1 is own *-4 by hollingsworth & vose co. , wa under\n",
      "contract *ich*-2 with lorillard * to make the cigarett filter . the\n",
      "find probabl will support those who *t*-6 argu that the u.s. should\n",
      "regul the class of asbesto includ crocidolit more stringent than the\n",
      "common kind of asbesto , chrysotil , found * in most school and other\n",
      "build , dr. talcott said 0 *t*-1 . the u.s. is one of the few industri\n",
      "nation that *t*-7 doe n't have a higher standard of regul for the\n",
      "smooth , needle-lik fiber such as crocidolit that *t*-1 are classifi\n",
      "*-5 as amphobil , accord to brook t. mossman , a professor of pathlog\n",
      "at the univers of vermont colleg of medicin . more common chrysotil\n",
      "fiber are curli and are more easili reject *-1 by the bodi , dr.\n",
      "mossman explain 0 *t*-2 . in juli , the environment protect agenc\n",
      "impos a gradual ban on virtual all use of asbesto . by 1997 , almost\n",
      "all remain use of cancer-caus asbesto will be outlaw *-6 . about 160\n",
      "worker at a factori that *t*-8 made paper for the kent filter were\n",
      "expos *-7 to asbesto in the 1950 . area of the factori *ich*-2 were\n",
      "particularli dusti where the crocidolit wa use *-8 *t*-1 . worker dump\n",
      "larg burlap sack of the import materi into a huge bin , pour in cotton\n",
      "and acet fiber and mechan mix the dri fiber in a process use * * to\n",
      "make filter . worker describ `` cloud of blue dust '' that *t*-1 hung\n",
      "over part of the factori , even though exhaust fan ventil the area .\n",
      "`` there 's no question that some of those worker and manag contract\n",
      "asbestos-rel diseas , '' said *t*-1 darrel phillip , vice presid of\n",
      "human resourc for hollingsworth & vose . `` but you have *-1 to recogn\n",
      "that these event took place 35 year ago . it ha no bear on our work\n",
      "forc today .\n",
      "**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# a demo of sample input output to Porter Stemmer\n",
    "nltk.stem.porter.demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", vice president of human resources for Hollingsworth & Vose . `` But you have *-1 to recognize that these events took place 35 years ago . It has no bearing on our work force today .\n",
      ", vice presid of human resourc for hollingsworth & vose . `` but you have *-1 to recogn that these event took place 35 year ago . it ha no bear on our work forc today .\n"
     ]
    }
   ],
   "source": [
    "print(', vice president of human resources for Hollingsworth & Vose . `` But you have *-1 to recognize that these events took place 35 years ago . It has no bearing on our work force today .')\n",
    "print(', vice presid of human resourc for hollingsworth & vose . `` but you have *-1 to recogn that these event took place 35 year ago . it ha no bear on our work forc today .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natur', 'u.', 'languag', 'process', 'nlp', 'fascin', 'field', 'focus', 'enabl', 'comput', 'understand', 'work', 'human', 'languag', 'to', 'often', 'break', 'sentenc', 'text', 'smaller', 'unit', 'call', 'token', 'these', 'token', 'simpl', 'individu', 'word', 'even', 'sub-word', 'unit', 'token', 'crucial', 'step', 'nlp', 'allow', 'us', 'analyz', 'process', 'text', 'data', 'effect', 'token', 'would', 'split', 'sentenc', 'individu', 'word', 'thi', 'breakdown', 'form', 'foundat', 'variou', 'nlp', 'task', 'sentiment', 'analysi', 'machin', 'translat', 'understand', 'token', 'first', 'step', 'unlock', 'power', 'nlp']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "# create an instance of the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "# lets use it to stem the words we had from before\n",
    "stemmed_words = [stemmer.stem(word) for word in stop_words_punctuation_removed_case_folded]\n",
    "print( stemmed_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization: \n",
    "Lemmatization as discussed in class targets to do things properly with the use of vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma. <br>\n",
    "In simple words lemmatization does the same work as stemming, the difference is that lemmatization returns a meaningful word.<br>\n",
    "<< Then why to use Stemming? >> <br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizers are also available in nltk.stem, lets test the use of WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'u.s', 'language', 'processing', 'nlp', 'fascinating', 'field', 'focuses', 'enabling', 'computers', 'understand', 'work', 'human', 'language', 'to', 'often', 'break', 'sentences', 'texts', 'smaller', 'units', 'called', 'tokens', 'these', 'tokens', 'simple', 'individual', 'words', 'even', 'sub-word', 'units', 'tokenization', 'crucial', 'step', 'nlp', 'allows', 'us', 'analyze', 'process', 'text', 'data', 'effectively', 'tokenization', 'would', 'split', 'sentences', 'individual', 'words', 'this', 'breakdown', 'forms', 'foundation', 'various', 'nlp', 'tasks', 'sentiment', 'analysis', 'machine', 'translation', 'understanding', 'tokenization', 'first', 'step', 'unlocking', 'power', 'nlp']\n",
      "['natural', 'u.s', 'language', 'processing', 'nlp', 'fascinating', 'field', 'focus', 'enabling', 'computer', 'understand', 'work', 'human', 'language', 'to', 'often', 'break', 'sentence', 'text', 'smaller', 'unit', 'called', 'token', 'these', 'token', 'simple', 'individual', 'word', 'even', 'sub-word', 'unit', 'tokenization', 'crucial', 'step', 'nlp', 'allows', 'u', 'analyze', 'process', 'text', 'data', 'effectively', 'tokenization', 'would', 'split', 'sentence', 'individual', 'word', 'this', 'breakdown', 'form', 'foundation', 'various', 'nlp', 'task', 'sentiment', 'analysis', 'machine', 'translation', 'understanding', 'tokenization', 'first', 'step', 'unlocking', 'power', 'nlp']\n",
      "['natur', 'u.', 'languag', 'process', 'nlp', 'fascin', 'field', 'focus', 'enabl', 'comput', 'understand', 'work', 'human', 'languag', 'to', 'often', 'break', 'sentenc', 'text', 'smaller', 'unit', 'call', 'token', 'these', 'token', 'simpl', 'individu', 'word', 'even', 'sub-word', 'unit', 'token', 'crucial', 'step', 'nlp', 'allow', 'us', 'analyz', 'process', 'text', 'data', 'effect', 'token', 'would', 'split', 'sentenc', 'individu', 'word', 'thi', 'breakdown', 'form', 'foundat', 'variou', 'nlp', 'task', 'sentiment', 'analysi', 'machin', 'translat', 'understand', 'token', 'first', 'step', 'unlock', 'power', 'nlp']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# lets use it to lemmatize the words we had from before, and print the results, stemming and original\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in stop_words_punctuation_removed_case_folded]\n",
    "print(stop_words_punctuation_removed_case_folded)\n",
    "print(lemmatized_words)\n",
    "print(stemmed_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Synonym/Antonym: \n",
    "Finding a synonym or antonym for a word, is useful if you need to identify words that mean the same in different context. For this, nltk corpus provides a wordnet corpus, which is a dictionary for the English Language designed for NLP tasks.<br>\n",
    "Lets check how does it work, by importing wordnet from nltk.corpus and cecking for some synonyms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'deplorable', 'sorry', 'distressing', 'pitiful', 'sad', 'lamentable'} {'glad'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "synonym_list = []\n",
    "antonym_list = []\n",
    "test_word = 'sad'\n",
    "for synonym in wordnet.synsets(test_word):\n",
    "    for i in synonym.lemmas(): ## Finding the lemma,matching and then appending synonyms\n",
    "        synonym_list.append(i.name())\n",
    "        if i.antonyms():\n",
    "            antonym_list.append(i.antonyms()[0].name())\n",
    "print(set(synonym_list), set(antonym_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech Tagging (POS): \n",
    "Stemming as discussed in class, is a process of converting a sentence to forms â€” a list of words, a list of tuples (where each tuple is having a form (word, tag)). The tag in the case is a part-of-speech tag and signifies whether the word is a noun, adjective, verb, and so on. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets_json to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping help\\tagsets_json.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('tagsets_json') #download tagsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do the POS tagging, we can do as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(: opening parenthesis\n",
      "    (\n",
      "): closing parenthesis\n",
      "    )\n",
      "*: negator\n",
      "    not n't\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ? ; ! :\n",
      ":: colon\n",
      "    :\n",
      "ABL: determiner/pronoun, pre-qualifier\n",
      "    quite such rather\n",
      "ABN: determiner/pronoun, pre-quantifier\n",
      "    all half many nary\n",
      "ABX: determiner/pronoun, double conjunction or pre-quantifier\n",
      "    both\n",
      "AP: determiner/pronoun, post-determiner\n",
      "    many other next more last former little several enough most least only\n",
      "    very few fewer past same Last latter less single plenty 'nough lesser\n",
      "    certain various manye next-to-last particular final previous present\n",
      "    nuf\n",
      "AP$: determiner/pronoun, post-determiner, genitive\n",
      "    other's\n",
      "AP+AP: determiner/pronoun, post-determiner, hyphenated pair\n",
      "    many-much\n",
      "AT: article\n",
      "    the an no a every th' ever' ye\n",
      "BE: verb 'to be', infinitive or imperative\n",
      "    be\n",
      "BED: verb 'to be', past tense, 2nd person singular or all persons plural\n",
      "    were\n",
      "BED*: verb 'to be', past tense, 2nd person singular or all persons plural, negated\n",
      "    weren't\n",
      "BEDZ: verb 'to be', past tense, 1st and 3rd person singular\n",
      "    was\n",
      "BEDZ*: verb 'to be', past tense, 1st and 3rd person singular, negated\n",
      "    wasn't\n",
      "BEG: verb 'to be', present participle or gerund\n",
      "    being\n",
      "BEM: verb 'to be', present tense, 1st person singular\n",
      "    am\n",
      "BEM*: verb 'to be', present tense, 1st person singular, negated\n",
      "    ain't\n",
      "BEN: verb 'to be', past participle\n",
      "    been\n",
      "BER: verb 'to be', present tense, 2nd person singular or all persons plural\n",
      "    are art\n",
      "BER*: verb 'to be', present tense, 2nd person singular or all persons plural, negated\n",
      "    aren't ain't\n",
      "BEZ: verb 'to be', present tense, 3rd person singular\n",
      "    is\n",
      "BEZ*: verb 'to be', present tense, 3rd person singular, negated\n",
      "    isn't ain't\n",
      "CC: conjunction, coordinating\n",
      "    and or but plus & either neither nor yet 'n' and/or minus an'\n",
      "CD: numeral, cardinal\n",
      "    two one 1 four 2 1913 71 74 637 1937 8 five three million 87-31 29-5\n",
      "    seven 1,119 fifty-three 7.5 billion hundred 125,000 1,700 60 100 six\n",
      "    ...\n",
      "CD$: numeral, cardinal, genitive\n",
      "    1960's 1961's .404's\n",
      "CS: conjunction, subordinating\n",
      "    that as after whether before while like because if since for than altho\n",
      "    until so unless though providing once lest s'posin' till whereas\n",
      "    whereupon supposing tho' albeit then so's 'fore\n",
      "DO: verb 'to do', uninflected present tense, infinitive or imperative\n",
      "    do dost\n",
      "DO*: verb 'to do', uninflected present tense or imperative, negated\n",
      "    don't\n",
      "DO+PPSS: verb 'to do', past or present tense + pronoun, personal, nominative, not 3rd person singular\n",
      "    d'you\n",
      "DOD: verb 'to do', past tense\n",
      "    did done\n",
      "DOD*: verb 'to do', past tense, negated\n",
      "    didn't\n",
      "DOZ: verb 'to do', present tense, 3rd person singular\n",
      "    does\n",
      "DOZ*: verb 'to do', present tense, 3rd person singular, negated\n",
      "    doesn't don't\n",
      "DT: determiner/pronoun, singular\n",
      "    this each another that 'nother\n",
      "DT$: determiner/pronoun, singular, genitive\n",
      "    another's\n",
      "DT+BEZ: determiner/pronoun + verb 'to be', present tense, 3rd person singular\n",
      "    that's\n",
      "DT+MD: determiner/pronoun + modal auxillary\n",
      "    that'll this'll\n",
      "DTI: determiner/pronoun, singular or plural\n",
      "    any some\n",
      "DTS: determiner/pronoun, plural\n",
      "    these those them\n",
      "DTS+BEZ: pronoun, plural + verb 'to be', present tense, 3rd person singular\n",
      "    them's\n",
      "DTX: determiner, pronoun or double conjunction\n",
      "    neither either one\n",
      "EX: existential there\n",
      "    there\n",
      "EX+BEZ: existential there + verb 'to be', present tense, 3rd person singular\n",
      "    there's\n",
      "EX+HVD: existential there + verb 'to have', past tense\n",
      "    there'd\n",
      "EX+HVZ: existential there + verb 'to have', present tense, 3rd person singular\n",
      "    there's\n",
      "EX+MD: existential there + modal auxillary\n",
      "    there'll there'd\n",
      "FW-*: foreign word: negator\n",
      "    pas non ne\n",
      "FW-AT: foreign word: article\n",
      "    la le el un die der ein keine eine das las les Il\n",
      "FW-AT+NN: foreign word: article + noun, singular, common\n",
      "    l'orchestre l'identite l'arcade l'ange l'assistance l'activite\n",
      "    L'Universite l'independance L'Union L'Unita l'osservatore\n",
      "FW-AT+NP: foreign word: article + noun, singular, proper\n",
      "    L'Astree L'Imperiale\n",
      "FW-BE: foreign word: verb 'to be', infinitive or imperative\n",
      "    sit\n",
      "FW-BER: foreign word: verb 'to be', present tense, 2nd person singular or all persons plural\n",
      "    sind sunt etes\n",
      "FW-BEZ: foreign word: verb 'to be', present tense, 3rd person singular\n",
      "    ist est\n",
      "FW-CC: foreign word: conjunction, coordinating\n",
      "    et ma mais und aber och nec y\n",
      "FW-CD: foreign word: numeral, cardinal\n",
      "    une cinq deux sieben unam zwei\n",
      "FW-CS: foreign word: conjunction, subordinating\n",
      "    bevor quam ma\n",
      "FW-DT: foreign word: determiner/pronoun, singular\n",
      "    hoc\n",
      "FW-DT+BEZ: foreign word: determiner + verb 'to be', present tense, 3rd person singular\n",
      "    c'est\n",
      "FW-DTS: foreign word: determiner/pronoun, plural\n",
      "    haec\n",
      "FW-HV: foreign word: verb 'to have', present tense, not 3rd person singular\n",
      "    habe\n",
      "FW-IN: foreign word: preposition\n",
      "    ad de en a par con dans ex von auf super post sine sur sub avec per\n",
      "    inter sans pour pendant in di\n",
      "FW-IN+AT: foreign word: preposition + article\n",
      "    della des du aux zur d'un del dell'\n",
      "FW-IN+NN: foreign word: preposition + noun, singular, common\n",
      "    d'etat d'hotel d'argent d'identite d'art\n",
      "FW-IN+NP: foreign word: preposition + noun, singular, proper\n",
      "    d'Yquem d'Eiffel\n",
      "FW-JJ: foreign word: adjective\n",
      "    avant Espagnol sinfonica Siciliana Philharmonique grand publique haute\n",
      "    noire bouffe Douce meme humaine bel serieuses royaux anticus presto\n",
      "    Sovietskaya Bayerische comique schwarzen ...\n",
      "FW-JJR: foreign word: adjective, comparative\n",
      "    fortiori\n",
      "FW-JJT: foreign word: adjective, superlative\n",
      "    optimo\n",
      "FW-NN: foreign word: noun, singular, common\n",
      "    ballet esprit ersatz mano chatte goutte sang Fledermaus oud def kolkhoz\n",
      "    roi troika canto boite blutwurst carne muzyka bonheur monde piece force\n",
      "    ...\n",
      "FW-NN$: foreign word: noun, singular, common, genitive\n",
      "    corporis intellectus arte's dei aeternitatis senioritatis curiae\n",
      "    patronne's chambre's\n",
      "FW-NNS: foreign word: noun, plural, common\n",
      "    al culpas vopos boites haflis kolkhozes augen tyrannis alpha-beta-\n",
      "    gammas metis banditos rata phis negociants crus Einsatzkommandos\n",
      "    kamikaze wohaws sabinas zorrillas palazzi engages coureurs corroborees\n",
      "    yori Ubermenschen ...\n",
      "FW-NP: foreign word: noun, singular, proper\n",
      "    Karshilama Dieu Rundfunk Afrique Espanol Afrika Spagna Gott Carthago\n",
      "    deus\n",
      "FW-NPS: foreign word: noun, plural, proper\n",
      "    Svenskarna Atlantes Dieux\n",
      "FW-NR: foreign word: noun, singular, adverbial\n",
      "    heute morgen aujourd'hui hoy\n",
      "FW-OD: foreign word: numeral, ordinal\n",
      "    18e 17e quintus\n",
      "FW-PN: foreign word: pronoun, nominal\n",
      "    hoc\n",
      "FW-PP$: foreign word: determiner, possessive\n",
      "    mea mon deras vos\n",
      "FW-PPL: foreign word: pronoun, singular, reflexive\n",
      "    se\n",
      "FW-PPL+VBZ: foreign word: pronoun, singular, reflexive + verb, present tense, 3rd person singular\n",
      "    s'excuse s'accuse\n",
      "FW-PPO: pronoun, personal, accusative\n",
      "    lui me moi mi\n",
      "FW-PPO+IN: foreign word: pronoun, personal, accusative + preposition\n",
      "    mecum tecum\n",
      "FW-PPS: foreign word: pronoun, personal, nominative, 3rd person singular\n",
      "    il\n",
      "FW-PPSS: foreign word: pronoun, personal, nominative, not 3rd person singular\n",
      "    ich vous sie je\n",
      "FW-PPSS+HV: foreign word: pronoun, personal, nominative, not 3rd person singular + verb 'to have', present tense, not 3rd person singular\n",
      "    j'ai\n",
      "FW-QL: foreign word: qualifier\n",
      "    minus\n",
      "FW-RB: foreign word: adverb\n",
      "    bas assai deja um wiederum cito velociter vielleicht simpliciter non zu\n",
      "    domi nuper sic forsan olim oui semper tout despues hors\n",
      "FW-RB+CC: foreign word: adverb + conjunction, coordinating\n",
      "    forisque\n",
      "FW-TO+VB: foreign word: infinitival to + verb, infinitive\n",
      "    d'entretenir\n",
      "FW-UH: foreign word: interjection\n",
      "    sayonara bien adieu arigato bonjour adios bueno tchalo ciao o\n",
      "FW-VB: foreign word: verb, present tense, not 3rd person singular, imperative or infinitive\n",
      "    nolo contendere vive fermate faciunt esse vade noli tangere dites duces\n",
      "    meminisse iuvabit gosaimasu voulez habla ksu'u'peli'afo lacheln miuchi\n",
      "    say allons strafe portant\n",
      "FW-VBD: foreign word: verb, past tense\n",
      "    stabat peccavi audivi\n",
      "FW-VBG: foreign word: verb, present participle or gerund\n",
      "    nolens volens appellant seq. obliterans servanda dicendi delenda\n",
      "FW-VBN: foreign word: verb, past participle\n",
      "    vue verstrichen rasa verboten engages\n",
      "FW-VBZ: foreign word: verb, present tense, 3rd person singular\n",
      "    gouverne sinkt sigue diapiace\n",
      "FW-WDT: foreign word: WH-determiner\n",
      "    quo qua quod que quok\n",
      "FW-WPO: foreign word: WH-pronoun, accusative\n",
      "    quibusdam\n",
      "FW-WPS: foreign word: WH-pronoun, nominative\n",
      "    qui\n",
      "HV: verb 'to have', uninflected present tense, infinitive or imperative\n",
      "    have hast\n",
      "HV*: verb 'to have', uninflected present tense or imperative, negated\n",
      "    haven't ain't\n",
      "HV+TO: verb 'to have', uninflected present tense + infinitival to\n",
      "    hafta\n",
      "HVD: verb 'to have', past tense\n",
      "    had\n",
      "HVD*: verb 'to have', past tense, negated\n",
      "    hadn't\n",
      "HVG: verb 'to have', present participle or gerund\n",
      "    having\n",
      "HVN: verb 'to have', past participle\n",
      "    had\n",
      "HVZ: verb 'to have', present tense, 3rd person singular\n",
      "    has hath\n",
      "HVZ*: verb 'to have', present tense, 3rd person singular, negated\n",
      "    hasn't ain't\n",
      "IN: preposition\n",
      "    of in for by considering to on among at through with under into\n",
      "    regarding than since despite according per before toward against as\n",
      "    after during including between without except upon out over ...\n",
      "IN+IN: preposition, hyphenated pair\n",
      "    f'ovuh\n",
      "IN+PPO: preposition + pronoun, personal, accusative\n",
      "    t'hi-im\n",
      "JJ: adjective\n",
      "    ecent over-all possible hard-fought favorable hard meager fit such\n",
      "    widespread outmoded inadequate ambiguous grand clerical effective\n",
      "    orderly federal foster general proportionate ...\n",
      "JJ$: adjective, genitive\n",
      "    Great's\n",
      "JJ+JJ: adjective, hyphenated pair\n",
      "    big-large long-far\n",
      "JJR: adjective, comparative\n",
      "    greater older further earlier later freer franker wider better deeper\n",
      "    firmer tougher faster higher bigger worse younger lighter nicer slower\n",
      "    happier frothier Greater newer Elder ...\n",
      "JJR+CS: adjective + conjunction, coordinating\n",
      "    lighter'n\n",
      "JJS: adjective, semantically superlative\n",
      "    top chief principal northernmost master key head main tops utmost\n",
      "    innermost foremost uppermost paramount topmost\n",
      "JJT: adjective, superlative\n",
      "    best largest coolest calmest latest greatest earliest simplest\n",
      "    strongest newest fiercest unhappiest worst youngest worthiest fastest\n",
      "    hottest fittest lowest finest smallest staunchest ...\n",
      "MD: modal auxillary\n",
      "    should may might will would must can could shall ought need wilt\n",
      "MD*: modal auxillary, negated\n",
      "    cannot couldn't wouldn't can't won't shouldn't shan't mustn't musn't\n",
      "MD+HV: modal auxillary + verb 'to have', uninflected form\n",
      "    shouldda musta coulda must've woulda could've\n",
      "MD+PPSS: modal auxillary + pronoun, personal, nominative, not 3rd person singular\n",
      "    willya\n",
      "MD+TO: modal auxillary + infinitival to\n",
      "    oughta\n",
      "NN: noun, singular, common\n",
      "    failure burden court fire appointment awarding compensation Mayor\n",
      "    interim committee fact effect airport management surveillance jail\n",
      "    doctor intern extern night weekend duty legislation Tax Office ...\n",
      "NN$: noun, singular, common, genitive\n",
      "    season's world's player's night's chapter's golf's football's\n",
      "    baseball's club's U.'s coach's bride's bridegroom's board's county's\n",
      "    firm's company's superintendent's mob's Navy's ...\n",
      "NN+BEZ: noun, singular, common + verb 'to be', present tense, 3rd person singular\n",
      "    water's camera's sky's kid's Pa's heat's throat's father's money's\n",
      "    undersecretary's granite's level's wife's fat's Knife's fire's name's\n",
      "    hell's leg's sun's roulette's cane's guy's kind's baseball's ...\n",
      "NN+HVD: noun, singular, common + verb 'to have', past tense\n",
      "    Pa'd\n",
      "NN+HVZ: noun, singular, common + verb 'to have', present tense, 3rd person singular\n",
      "    guy's Knife's boat's summer's rain's company's\n",
      "NN+IN: noun, singular, common + preposition\n",
      "    buncha\n",
      "NN+MD: noun, singular, common + modal auxillary\n",
      "    cowhand'd sun'll\n",
      "NN+NN: noun, singular, common, hyphenated pair\n",
      "    stomach-belly\n",
      "NNS: noun, plural, common\n",
      "    irregularities presentments thanks reports voters laws legislators\n",
      "    years areas adjustments chambers $100 bonds courts sales details raises\n",
      "    sessions members congressmen votes polls calls ...\n",
      "NNS$: noun, plural, common, genitive\n",
      "    taxpayers' children's members' States' women's cutters' motorists'\n",
      "    steelmakers' hours' Nations' lawyers' prisoners' architects' tourists'\n",
      "    Employers' secretaries' Rogues' ...\n",
      "NNS+MD: noun, plural, common + modal auxillary\n",
      "    duds'd oystchers'll\n",
      "NP: noun, singular, proper\n",
      "    Fulton Atlanta September-October Durwood Pye Ivan Allen Jr. Jan.\n",
      "    Alpharetta Grady William B. Hartsfield Pearl Williams Aug. Berry J. M.\n",
      "    Cheshire Griffin Opelika Ala. E. Pelham Snodgrass ...\n",
      "NP$: noun, singular, proper, genitive\n",
      "    Green's Landis' Smith's Carreon's Allison's Boston's Spahn's Willie's\n",
      "    Mickey's Milwaukee's Mays' Howsam's Mantle's Shaw's Wagner's Rickey's\n",
      "    Shea's Palmer's Arnold's Broglio's ...\n",
      "NP+BEZ: noun, singular, proper + verb 'to be', present tense, 3rd person singular\n",
      "    W.'s Ike's Mack's Jack's Kate's Katharine's Black's Arthur's Seaton's\n",
      "    Buckhorn's Breed's Penny's Rob's Kitty's Blackwell's Myra's Wally's\n",
      "    Lucille's Springfield's Arlene's\n",
      "NP+HVZ: noun, singular, proper + verb 'to have', present tense, 3rd person singular\n",
      "    Bill's Guardino's Celie's Skolman's Crosson's Tim's Wally's\n",
      "NP+MD: noun, singular, proper + modal auxillary\n",
      "    Gyp'll John'll\n",
      "NPS: noun, plural, proper\n",
      "    Chases Aderholds Chapelles Armisteads Lockies Carbones French Marskmen\n",
      "    Toppers Franciscans Romans Cadillacs Masons Blacks Catholics British\n",
      "    Dixiecrats Mississippians Congresses ...\n",
      "NPS$: noun, plural, proper, genitive\n",
      "    Republicans' Orioles' Birds' Yanks' Redbirds' Bucs' Yankees' Stevenses'\n",
      "    Geraghtys' Burkes' Wackers' Achaeans' Dresbachs' Russians' Democrats'\n",
      "    Gershwins' Adventists' Negroes' Catholics' ...\n",
      "NR: noun, singular, adverbial\n",
      "    Friday home Wednesday Tuesday Monday Sunday Thursday yesterday tomorrow\n",
      "    tonight West East Saturday west left east downtown north northeast\n",
      "    southeast northwest North South right ...\n",
      "NR$: noun, singular, adverbial, genitive\n",
      "    Saturday's Monday's yesterday's tonight's tomorrow's Sunday's\n",
      "    Wednesday's Friday's today's Tuesday's West's Today's South's\n",
      "NR+MD: noun, singular, adverbial + modal auxillary\n",
      "    today'll\n",
      "NRS: noun, plural, adverbial\n",
      "    Sundays Mondays Saturdays Wednesdays Souths Fridays\n",
      "OD: numeral, ordinal\n",
      "    first 13th third nineteenth 2d 61st second sixth eighth ninth twenty-\n",
      "    first eleventh 50th eighteenth- Thirty-ninth 72nd 1/20th twentieth\n",
      "    mid-19th thousandth 350th sixteenth 701st ...\n",
      "PN: pronoun, nominal\n",
      "    none something everything one anyone nothing nobody everybody everyone\n",
      "    anybody anything someone no-one nothin\n",
      "PN$: pronoun, nominal, genitive\n",
      "    one's someone's anybody's nobody's everybody's anyone's everyone's\n",
      "PN+BEZ: pronoun, nominal + verb 'to be', present tense, 3rd person singular\n",
      "    nothing's everything's somebody's nobody's someone's\n",
      "PN+HVD: pronoun, nominal + verb 'to have', past tense\n",
      "    nobody'd\n",
      "PN+HVZ: pronoun, nominal + verb 'to have', present tense, 3rd person singular\n",
      "    nobody's somebody's one's\n",
      "PN+MD: pronoun, nominal + modal auxillary\n",
      "    someone'll somebody'll anybody'd\n",
      "PP$: determiner, possessive\n",
      "    our its his their my your her out thy mine thine\n",
      "PP$$: pronoun, possessive\n",
      "    ours mine his hers theirs yours\n",
      "PPL: pronoun, singular, reflexive\n",
      "    itself himself myself yourself herself oneself ownself\n",
      "PPLS: pronoun, plural, reflexive\n",
      "    themselves ourselves yourselves\n",
      "PPO: pronoun, personal, accusative\n",
      "    them it him me us you 'em her thee we'uns\n",
      "PPS: pronoun, personal, nominative, 3rd person singular\n",
      "    it he she thee\n",
      "PPS+BEZ: pronoun, personal, nominative, 3rd person singular + verb 'to be', present tense, 3rd person singular\n",
      "    it's he's she's\n",
      "PPS+HVD: pronoun, personal, nominative, 3rd person singular + verb 'to have', past tense\n",
      "    she'd he'd it'd\n",
      "PPS+HVZ: pronoun, personal, nominative, 3rd person singular + verb 'to have', present tense, 3rd person singular\n",
      "    it's he's she's\n",
      "PPS+MD: pronoun, personal, nominative, 3rd person singular + modal auxillary\n",
      "    he'll she'll it'll he'd it'd she'd\n",
      "PPSS: pronoun, personal, nominative, not 3rd person singular\n",
      "    they we I you ye thou you'uns\n",
      "PPSS+BEM: pronoun, personal, nominative, not 3rd person singular + verb 'to be', present tense, 1st person singular\n",
      "    I'm Ahm\n",
      "PPSS+BER: pronoun, personal, nominative, not 3rd person singular + verb 'to be', present tense, 2nd person singular or all persons plural\n",
      "    we're you're they're\n",
      "PPSS+BEZ: pronoun, personal, nominative, not 3rd person singular + verb 'to be', present tense, 3rd person singular\n",
      "    you's\n",
      "PPSS+BEZ*: pronoun, personal, nominative, not 3rd person singular + verb 'to be', present tense, 3rd person singular, negated\n",
      "    'tain't\n",
      "PPSS+HV: pronoun, personal, nominative, not 3rd person singular + verb 'to have', uninflected present tense\n",
      "    I've we've they've you've\n",
      "PPSS+HVD: pronoun, personal, nominative, not 3rd person singular + verb 'to have', past tense\n",
      "    I'd you'd we'd they'd\n",
      "PPSS+MD: pronoun, personal, nominative, not 3rd person singular + modal auxillary\n",
      "    you'll we'll I'll we'd I'd they'll they'd you'd\n",
      "PPSS+VB: pronoun, personal, nominative, not 3rd person singular + verb 'to verb', uninflected present tense\n",
      "    y'know\n",
      "QL: qualifier, pre\n",
      "    well less very most so real as highly fundamentally even how much\n",
      "    remarkably somewhat more completely too thus ill deeply little overly\n",
      "    halfway almost impossibly far severly such ...\n",
      "QLP: qualifier, post\n",
      "    indeed enough still 'nuff\n",
      "RB: adverb\n",
      "    only often generally also nevertheless upon together back newly no\n",
      "    likely meanwhile near then heavily there apparently yet outright fully\n",
      "    aside consistently specifically formally ever just ...\n",
      "RB$: adverb, genitive\n",
      "    else's\n",
      "RB+BEZ: adverb + verb 'to be', present tense, 3rd person singular\n",
      "    here's there's\n",
      "RB+CS: adverb + conjunction, coordinating\n",
      "    well's soon's\n",
      "RBR: adverb, comparative\n",
      "    further earlier better later higher tougher more harder longer sooner\n",
      "    less faster easier louder farther oftener nearer cheaper slower tighter\n",
      "    lower worse heavier quicker ...\n",
      "RBR+CS: adverb, comparative + conjunction, coordinating\n",
      "    more'n\n",
      "RBT: adverb, superlative\n",
      "    most best highest uppermost nearest brightest hardest fastest deepest\n",
      "    farthest loudest ...\n",
      "RN: adverb, nominal\n",
      "    here afar then\n",
      "RP: adverb, particle\n",
      "    up out off down over on in about through across after\n",
      "RP+IN: adverb, particle + preposition\n",
      "    out'n outta\n",
      "TO: infinitival to\n",
      "    to t'\n",
      "TO+VB: infinitival to + verb, infinitive\n",
      "    t'jawn t'lah\n",
      "UH: interjection\n",
      "    Hurrah bang whee hmpf ah goodbye oops oh-the-pain-of-it ha crunch say\n",
      "    oh why see well hello lo alas tarantara rum-tum-tum gosh hell keerist\n",
      "    Jesus Keeeerist boy c'mon 'mon goddamn bah hoo-pig damn ...\n",
      "VB: verb, base: uninflected present, imperative or infinitive\n",
      "    investigate find act follow inure achieve reduce take remedy re-set\n",
      "    distribute realize disable feel receive continue place protect\n",
      "    eliminate elaborate work permit run enter force ...\n",
      "VB+AT: verb, base: uninflected present or infinitive + article\n",
      "    wanna\n",
      "VB+IN: verb, base: uninflected present, imperative or infinitive + preposition\n",
      "    lookit\n",
      "VB+JJ: verb, base: uninflected present, imperative or infinitive + adjective\n",
      "    die-dead\n",
      "VB+PPO: verb, uninflected present tense + pronoun, personal, accusative\n",
      "    let's lemme gimme\n",
      "VB+RP: verb, imperative + adverbial particle\n",
      "    g'ahn c'mon\n",
      "VB+TO: verb, base: uninflected present, imperative or infinitive + infinitival to\n",
      "    wanta wanna\n",
      "VB+VB: verb, base: uninflected present, imperative or infinitive; hypenated pair\n",
      "    say-speak\n",
      "VBD: verb, past tense\n",
      "    said produced took recommended commented urged found added praised\n",
      "    charged listed became announced brought attended wanted voted defeated\n",
      "    received got stood shot scheduled feared promised made ...\n",
      "VBG: verb, present participle or gerund\n",
      "    modernizing improving purchasing Purchasing lacking enabling pricing\n",
      "    keeping getting picking entering voting warning making strengthening\n",
      "    setting neighboring attending participating moving ...\n",
      "VBG+TO: verb, present participle + infinitival to\n",
      "    gonna\n",
      "VBN: verb, past participle\n",
      "    conducted charged won received studied revised operated accepted\n",
      "    combined experienced recommended effected granted seen protected\n",
      "    adopted retarded notarized selected composed gotten printed ...\n",
      "VBN+TO: verb, past participle + infinitival to\n",
      "    gotta\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    deserves believes receives takes goes expires says opposes starts\n",
      "    permits expects thinks faces votes teaches holds calls fears spends\n",
      "    collects backs eliminates sets flies gives seeks reads ...\n",
      "WDT: WH-determiner\n",
      "    which what whatever whichever whichever-the-hell\n",
      "WDT+BER: WH-determiner + verb 'to be', present tense, 2nd person singular or all persons plural\n",
      "    what're\n",
      "WDT+BER+PP: WH-determiner + verb 'to be', present, 2nd person singular or all persons plural + pronoun, personal, nominative, not 3rd person singular\n",
      "    whaddya\n",
      "WDT+BEZ: WH-determiner + verb 'to be', present tense, 3rd person singular\n",
      "    what's\n",
      "WDT+DO+PPS: WH-determiner + verb 'to do', uninflected present tense + pronoun, personal, nominative, not 3rd person singular\n",
      "    whaddya\n",
      "WDT+DOD: WH-determiner + verb 'to do', past tense\n",
      "    what'd\n",
      "WDT+HVZ: WH-determiner + verb 'to have', present tense, 3rd person singular\n",
      "    what's\n",
      "WP$: WH-pronoun, genitive\n",
      "    whose whosever\n",
      "WPO: WH-pronoun, accusative\n",
      "    whom that who\n",
      "WPS: WH-pronoun, nominative\n",
      "    that who whoever whosoever what whatsoever\n",
      "WPS+BEZ: WH-pronoun, nominative + verb 'to be', present, 3rd person singular\n",
      "    that's who's\n",
      "WPS+HVD: WH-pronoun, nominative + verb 'to have', past tense\n",
      "    who'd\n",
      "WPS+HVZ: WH-pronoun, nominative + verb 'to have', present tense, 3rd person singular\n",
      "    who's that's\n",
      "WPS+MD: WH-pronoun, nominative + modal auxillary\n",
      "    who'll that'd who'd that'll\n",
      "WQL: WH-qualifier\n",
      "    however how\n",
      "WRB: WH-adverb\n",
      "    however when where why whereby wherever how whenever whereon wherein\n",
      "    wherewith wheare wherefore whereof howsabout\n",
      "WRB+BER: WH-adverb + verb 'to be', present, 2nd person singular or all persons plural\n",
      "    where're\n",
      "WRB+BEZ: WH-adverb + verb 'to be', present, 3rd person singular\n",
      "    how's where's\n",
      "WRB+DO: WH-adverb + verb 'to do', present, not 3rd person singular\n",
      "    howda\n",
      "WRB+DOD: WH-adverb + verb 'to do', past tense\n",
      "    where'd how'd\n",
      "WRB+DOD*: WH-adverb + verb 'to do', past tense, negated\n",
      "    whyn't\n",
      "WRB+DOZ: WH-adverb + verb 'to do', present tense, 3rd person singular\n",
      "    how's\n",
      "WRB+IN: WH-adverb + preposition\n",
      "    why'n\n",
      "WRB+MD: WH-adverb + modal auxillary\n",
      "    where'd\n"
     ]
    }
   ],
   "source": [
    "nltk.help.brown_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Natural', 'JJ'), ('@', 'NNP'), ('(', '('), ('U.S', 'NNP'), ('.', '.'), ('Language', 'NN'), ('Processing', 'NN'), (',', ','), ('or', 'CC'), ('NLP', 'NNP'), (',', ','), ('is', 'VBZ'), ('a', 'DT'), ('fascinating', 'JJ'), ('field', 'NN'), ('that', 'WDT'), ('focuses', 'VBZ'), ('on', 'IN'), ('enabling', 'VBG'), ('computers', 'NNS'), ('to', 'TO'), ('understand', 'VB'), ('and', 'CC'), ('work', 'VB'), ('with', 'IN'), ('human', 'JJ'), ('language', 'NN'), ('.', '.'), ('To', 'TO'), ('do', 'VB'), ('this', 'DT'), (',', ','), ('we', 'PRP'), ('often', 'RB'), ('#', '#'), ('break', 'VB'), ('down', 'RP'), ('sentences', 'NNS'), ('or', 'CC'), ('texts', 'VB'), ('into', 'IN'), ('smaller', 'JJR'), ('units', 'NNS'), ('called', 'VBD'), ('tokens', 'NNS'), ('.', '.'), ('These', 'DT'), ('tokens', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('as', 'IN'), ('@', 'JJ'), ('simple', 'NN'), ('as', 'IN'), ('individual', 'JJ'), ('words', 'NNS'), ('or', 'CC'), ('even', 'RB'), ('sub-word', 'JJ'), ('units', 'NNS'), ('.', '.'), ('Tokenization', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('crucial', 'JJ'), ('step', 'NN'), ('in', 'IN'), ('NLP', 'NNP'), ('because', 'IN'), ('it', 'PRP'), ('allows', 'VBZ'), ('us', 'PRP'), ('to', 'TO'), ('analyze', 'VB'), ('and', 'CC'), ('process', 'VB'), ('text', 'NN'), ('data', 'NNS'), ('more', 'RBR'), ('effectively', 'RB'), ('.', '.'), ('Tokenization', 'NNP'), ('would', 'MD'), ('split', 'VB'), ('sentences', 'NNS'), ('into', 'IN'), ('individual', 'JJ'), ('words', 'NNS'), ('.', '.'), ('This', 'DT'), ('breakdown', 'NN'), ('forms', 'VBZ'), ('the', 'DT'), ('foundation', 'NN'), ('for', 'IN'), ('various', 'JJ'), ('NLP', 'NNP'), ('tasks', 'NNS'), (',', ','), ('from', 'IN'), ('sentiment', 'NN'), ('analysis', 'NN'), ('to', 'TO'), ('machine', 'NN'), ('translation', 'NN'), ('.', '.'), ('Understanding', 'VBG'), ('tokenization', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('first', 'JJ'), ('step', 'NN'), ('in', 'IN'), ('unlocking', 'VBG'), ('the', 'DT'), ('power', 'NN'), ('of', 'IN'), ('NLP', 'NNP'), ('.', '.')]\n",
      "[('natural', 'JJ'), ('u.s', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('nlp', 'JJ'), ('fascinating', 'JJ'), ('field', 'NN'), ('focus', 'NN'), ('enabling', 'VBG'), ('computer', 'NN'), ('understand', 'NN'), ('work', 'NN'), ('human', 'JJ'), ('language', 'NN'), ('to', 'TO'), ('often', 'RB'), ('break', 'VB'), ('sentence', 'NN'), ('text', 'IN'), ('smaller', 'JJR'), ('unit', 'NN'), ('called', 'VBD'), ('token', 'JJ'), ('these', 'DT'), ('token', 'JJ'), ('simple', 'NN'), ('individual', 'JJ'), ('word', 'NN'), ('even', 'RB'), ('sub-word', 'JJ'), ('unit', 'NN'), ('tokenization', 'NN'), ('crucial', 'JJ'), ('step', 'NN'), ('nlp', 'RB'), ('allows', 'VBZ'), ('u', 'JJ'), ('analyze', 'JJ'), ('process', 'NN'), ('text', 'NN'), ('data', 'NNS'), ('effectively', 'RB'), ('tokenization', 'NN'), ('would', 'MD'), ('split', 'VB'), ('sentence', 'NN'), ('individual', 'JJ'), ('word', 'NN'), ('this', 'DT'), ('breakdown', 'JJ'), ('form', 'NN'), ('foundation', 'NN'), ('various', 'JJ'), ('nlp', 'JJ'), ('task', 'NN'), ('sentiment', 'NN'), ('analysis', 'NN'), ('machine', 'NN'), ('translation', 'NN'), ('understanding', 'VBG'), ('tokenization', 'NN'), ('first', 'JJ'), ('step', 'NN'), ('unlocking', 'VBG'), ('power', 'NN'), ('nlp', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# We apply POS tagging to words before any preprocessing, check the difference\n",
    "print(nltk.pos_tag(word_tokenize(test_string)))\n",
    "print(nltk.pos_tag(word_tokenize(\" \".join(lemmatized_words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('natur', 'JJ'),\n",
       " ('u.', 'JJ'),\n",
       " ('languag', 'NN'),\n",
       " ('process', 'NN'),\n",
       " ('nlp', 'JJ'),\n",
       " ('fascin', 'JJ'),\n",
       " ('field', 'NN'),\n",
       " ('focus', 'NN'),\n",
       " ('enabl', 'VBD'),\n",
       " ('comput', 'JJ'),\n",
       " ('understand', 'JJ'),\n",
       " ('work', 'NN'),\n",
       " ('human', 'JJ'),\n",
       " ('languag', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('often', 'RB'),\n",
       " ('break', 'VB'),\n",
       " ('sentenc', 'JJ'),\n",
       " ('text', 'JJ'),\n",
       " ('smaller', 'JJR'),\n",
       " ('unit', 'NN'),\n",
       " ('call', 'NN'),\n",
       " ('token', 'IN'),\n",
       " ('these', 'DT'),\n",
       " ('token', 'JJ'),\n",
       " ('simpl', 'NN'),\n",
       " ('individu', 'NN'),\n",
       " ('word', 'NN'),\n",
       " ('even', 'RB'),\n",
       " ('sub-word', 'JJ'),\n",
       " ('unit', 'NN'),\n",
       " ('token', 'JJ'),\n",
       " ('crucial', 'JJ'),\n",
       " ('step', 'NN'),\n",
       " ('nlp', 'RB'),\n",
       " ('allow', 'VB'),\n",
       " ('us', 'PRP'),\n",
       " ('analyz', 'JJ'),\n",
       " ('process', 'NN'),\n",
       " ('text', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('effect', 'NN'),\n",
       " ('token', 'NN'),\n",
       " ('would', 'MD'),\n",
       " ('split', 'VB'),\n",
       " ('sentenc', 'JJ'),\n",
       " ('individu', 'NN'),\n",
       " ('word', 'NN'),\n",
       " ('thi', 'NN'),\n",
       " ('breakdown', 'JJ'),\n",
       " ('form', 'NN'),\n",
       " ('foundat', 'NN'),\n",
       " ('variou', 'NN'),\n",
       " ('nlp', 'JJ'),\n",
       " ('task', 'NN'),\n",
       " ('sentiment', 'NN'),\n",
       " ('analysi', 'NN'),\n",
       " ('machin', 'JJ'),\n",
       " ('translat', 'JJ'),\n",
       " ('understand', 'NN'),\n",
       " ('token', 'NN'),\n",
       " ('first', 'JJ'),\n",
       " ('step', 'NN'),\n",
       " ('unlock', 'JJ'),\n",
       " ('power', 'NN'),\n",
       " ('nlp', 'NN')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCN8010_classic_ml",
   "language": "python",
   "name": "cscn8010_classic_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
